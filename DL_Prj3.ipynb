{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDMN-hNGfBjA",
        "outputId": "7e380a8e-221f-4285-b496-3f40bf81bcf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZOPyxYR0rkv",
        "outputId": "99806bd3-0cc6-4fa1-8d64-a3597caec3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkk-QM2k1lzV",
        "outputId": "32955a4e-63bd-4b61-b775-b1374fbd3353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TextClassificationAttention\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/TextClassificationAttention\n",
        "#!kaggle datasets download adityajn105/glove6b50d\n",
        "#!kaggle datasets download dushyantv/consumer_complaints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6zE_6crpcSY"
      },
      "outputs": [],
      "source": [
        "#!unzip consumer_complaints.zip\n",
        "#!unzip glove6b50d.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFsdLDB0qG_A",
        "outputId": "6f5705a8-51fb-4617-a7f6-db641b0914b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
            "       'Consumer Complaint', 'Company Public Response', 'Company', 'State',\n",
            "       'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via',\n",
            "       'Date Sent to Company', 'Company Response to Consumer',\n",
            "       'Timely response?', 'Consumer disputed?', 'Complaint ID',\n",
            "       'Unnamed: 18'],\n",
            "      dtype='object')\n",
            "  Date received           Product     Sub-product  \\\n",
            "0    03-12-2014          Mortgage  Other mortgage   \n",
            "1    10-01-2016  Credit reporting             NaN   \n",
            "2    10/17/2016     Consumer Loan    Vehicle loan   \n",
            "3    06-08-2014       Credit card             NaN   \n",
            "4    09/13/2014   Debt collection     Credit card   \n",
            "\n",
            "                                      Issue                   Sub-issue  \\\n",
            "0  Loan modification,collection,foreclosure                         NaN   \n",
            "1    Incorrect information on credit report              Account status   \n",
            "2                Managing the loan or lease                         NaN   \n",
            "3                                Bankruptcy                         NaN   \n",
            "4                     Communication tactics  Frequent or repeated calls   \n",
            "\n",
            "                                  Consumer Complaint  \\\n",
            "0                                                NaN   \n",
            "1  I have outdated information on my credit repor...   \n",
            "2  I purchased a new car on XXXX XXXX. The car de...   \n",
            "3                                                NaN   \n",
            "4                                                NaN   \n",
            "\n",
            "                             Company Public Response  \\\n",
            "0                                                NaN   \n",
            "1  Company has responded to the consumer and the ...   \n",
            "2                                                NaN   \n",
            "3                                                NaN   \n",
            "4                                                NaN   \n",
            "\n",
            "                                  Company State ZIP code            Tags  \\\n",
            "0                    M&T BANK CORPORATION    MI    48382             NaN   \n",
            "1  TRANSUNION INTERMEDIATE HOLDINGS, INC.    AL    352XX             NaN   \n",
            "2          CITIZENS FINANCIAL GROUP, INC.    PA    177XX  Older American   \n",
            "3                AMERICAN EXPRESS COMPANY    ID    83854  Older American   \n",
            "4                          CITIBANK, N.A.    VA    23233             NaN   \n",
            "\n",
            "  Consumer consent provided? Submitted via Date Sent to Company  \\\n",
            "0                        NaN      Referral           03/17/2014   \n",
            "1           Consent provided           Web           10-05-2016   \n",
            "2           Consent provided           Web           10/20/2016   \n",
            "3                        NaN           Web           06-10-2014   \n",
            "4                        NaN           Web           09/13/2014   \n",
            "\n",
            "  Company Response to Consumer Timely response? Consumer disputed?  \\\n",
            "0      Closed with explanation              Yes                 No   \n",
            "1      Closed with explanation              Yes                 No   \n",
            "2      Closed with explanation              Yes                 No   \n",
            "3      Closed with explanation              Yes                Yes   \n",
            "4      Closed with explanation              Yes                Yes   \n",
            "\n",
            "   Complaint ID  Unnamed: 18  \n",
            "0        759217          NaN  \n",
            "1       2141773          NaN  \n",
            "2       2163100          NaN  \n",
            "3        885638          NaN  \n",
            "4       1027760          NaN  \n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/TextClassificationAttention/Consumer_Complaints.csv\")\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uwoUzvvnvn6"
      },
      "source": [
        "# Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVKD5cmEh50J"
      },
      "outputs": [],
      "source": [
        "lr = 0.0005\n",
        "vec_len = 50\n",
        "seq_len = 20\n",
        "num_epochs = 50\n",
        "label_col = \"Product\"\n",
        "tokens_path = \"/content/drive/MyDrive/TextClassificationAttention/tokens.pkl\"\n",
        "labels_path = \"/content/drive/MyDrive/TextClassificationAttention/labels.pkl\"\n",
        "data_path = \"/content/drive/MyDrive/TextClassificationAttention/Consumer_Complaints.csv\"\n",
        "model_path = \"/content/drive/MyDrive/TextClassificationAttention/attention.pth\"\n",
        "vocabulary_path = \"/content/drive/MyDrive/TextClassificationAttention/vocabulary.pkl\"\n",
        "embeddings_path = \"/content/drive/MyDrive/TextClassificationAttention/embeddings.pkl\"\n",
        "glove_vector_path = \"/content/drive/MyDrive/TextClassificationAttention/glove.6B.50d.txt\"\n",
        "text_col_name = \"Consumer Complaint\"\n",
        "label_encoder_path = \"/content/drive/MyDrive/TextClassificationAttention/label_encoder.pkl\"\n",
        "product_map = {'Vehicle loan or lease': 'vehicle_loan',\n",
        "               'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n",
        "               'Credit card or prepaid card': 'card',\n",
        "               'Money transfer, virtual currency, or money service': 'money_transfer',\n",
        "               'virtual currency': 'money_transfer',\n",
        "               'Mortgage': 'mortgage',\n",
        "               'Payday loan, title loan, or personal loan': 'loan',\n",
        "               'Debt collection': 'debt_collection',\n",
        "               'Checking or savings account': 'savings_account',\n",
        "               'Credit card': 'card',\n",
        "               'Bank account or service': 'savings_account',\n",
        "               'Credit reporting': 'credit_report',\n",
        "               'Prepaid card': 'card',\n",
        "               'Payday loan': 'loan',\n",
        "               'Other financial service': 'others',\n",
        "               'Virtual currency': 'money_transfer',\n",
        "               'Student loan': 'loan',\n",
        "               'Consumer Loan': 'loan',\n",
        "               'Money transfers': 'money_transfer'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM1YA97hiscE"
      },
      "outputs": [],
      "source": [
        "def save_file(name, obj):\n",
        "    \"\"\"\n",
        "    Function to save an object as pickle file\n",
        "    \"\"\"\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def load_file(name):\n",
        "    \"\"\"\n",
        "    Function to load a pickle object\n",
        "    \"\"\"\n",
        "    return pickle.load(open(name, \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1RQt1dykFV3"
      },
      "source": [
        "Glove embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWwmv6z8kDFU"
      },
      "outputs": [],
      "source": [
        "with open(glove_vector_path, \"rt\") as f:\n",
        "    emb = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR6H4hVRkhOg"
      },
      "outputs": [],
      "source": [
        "vocabulary, embeddings = [], []\n",
        "\n",
        "for item in emb:\n",
        "    vocabulary.append(item.split()[0])\n",
        "    embeddings.append(item.split()[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejmRw48nkj88"
      },
      "outputs": [],
      "source": [
        "embeddings = np.array(embeddings, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZencTawkkmnn"
      },
      "outputs": [],
      "source": [
        "vocabulary = [\"<pad>\", \"<unk>\"] + vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n0bwX8VkpjB"
      },
      "outputs": [],
      "source": [
        "embeddings = np.vstack([np.ones(50, dtype=np.float32),\n",
        "                        np.mean(embeddings, axis=0),\n",
        "                        embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fppxCf_3kvVd"
      },
      "outputs": [],
      "source": [
        "save_file(embeddings_path, embeddings)\n",
        "save_file(vocabulary_path, vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DleSFNb57lKD"
      },
      "source": [
        "# Process text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYVm7W27o-q"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kqxs2NO7rYh"
      },
      "outputs": [],
      "source": [
        "data.dropna(subset=[text_col_name], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar0rwndX7skJ"
      },
      "outputs": [],
      "source": [
        "data.replace({label_col: product_map}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTz5jq6Q5lfE"
      },
      "source": [
        "# Encode labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEuvCx6C5pyE"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(data[label_col])\n",
        "labels = label_encoder.transform(data[label_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n9PxzZf5raD"
      },
      "outputs": [],
      "source": [
        "save_file(labels_path, labels)\n",
        "save_file(label_encoder_path, label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emAUjvb36Mh8"
      },
      "source": [
        "# Process text column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR_K-qOp6Rxl"
      },
      "outputs": [],
      "source": [
        "input_text = list(data[text_col_name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7aSso_v6qE8",
        "outputId": "cba0b59d-c5e9-41bd-bd87-c8ebaa9dfb63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "277814"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPuy4-Ax6vs6"
      },
      "source": [
        "# Convert to lowecase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl3gjx_c6zr6",
        "outputId": "b6e9f011-cd45-4055-8952-1a65ad7da572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:00<00:00, 737053.34it/s]\n"
          ]
        }
      ],
      "source": [
        "input_text = [i.lower() for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rosMMWOA63eE"
      },
      "source": [
        "# Remove punctuations except apostrophe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKKRDzmh7DET",
        "outputId": "309e2797-f6ed-47d5-81a5-b2b944233d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:13<00:00, 21047.88it/s]\n"
          ]
        }
      ],
      "source": [
        "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i)\n",
        "              for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poZ7ioFL7Hu3"
      },
      "source": [
        "# remove Numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SRzcNQT7HNL",
        "outputId": "6aefd038-3ae4-43ee-efed-1726d891b91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:08<00:00, 31598.33it/s]\n"
          ]
        }
      ],
      "source": [
        "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4nPMHf7rl6"
      },
      "source": [
        "# Remove more than one consecutive instance of 'x'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEY8PBRj7u0S",
        "outputId": "e6644069-7c28-4721-f87a-a2e63f2aea32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:05<00:00, 47203.01it/s]\n"
          ]
        }
      ],
      "source": [
        "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP16KhR27wcr"
      },
      "source": [
        "# Remove multiple spaces with single space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45HVRclx7y5L",
        "outputId": "9ece137f-06d7-4a3c-d947-d4cfceab2b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:22<00:00, 12217.23it/s]\n"
          ]
        }
      ],
      "source": [
        "nput_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MIRMH4k70s7"
      },
      "source": [
        "# Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDVCdPjrIpq9",
        "outputId": "9928775a-f526-418b-a19f-4d5f76802dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#tokens = [word_tokenize(t) for t in tqdm(input_text, mininterval=60)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTiWqkj277Si"
      },
      "source": [
        "# Take the first 20 tokens in each complaint text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mNMkQFG7_Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba11c094-8696-47fe-acf1-64c8b8eb2f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [00:03<00:00, 73475.70it/s] \n"
          ]
        }
      ],
      "source": [
        "tokens = [i[:20] if len(i) > 19 else ['<pad>'] * (20 - len(i)) + i\n",
        "          for i in tqdm(tokens)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAU4GSVI8Byz"
      },
      "source": [
        "# Convert tokens to integer indices from vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2APNC9Me8EKS"
      },
      "outputs": [],
      "source": [
        "def token_index(tokens, vocabulary, missing='<unk>'):\n",
        "    \"\"\"\n",
        "    :param tokens: List of word tokens\n",
        "    :param vocabulary: All words in the embeddings\n",
        "    :param missing: Token for words not present in the vocabulary\n",
        "    :return: List of integers representing the word tokens\n",
        "    \"\"\"\n",
        "    idx_token = []\n",
        "    for text in tqdm(tokens):\n",
        "        idx_text = []\n",
        "        for token in text:\n",
        "            if token in vocabulary:\n",
        "                idx_text.append(vocabulary.index(token))\n",
        "            else:\n",
        "                idx_text.append(vocabulary.index(missing))\n",
        "        idx_token.append(idx_text)\n",
        "    return idx_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PovRhgP_mXc"
      },
      "source": [
        "# save the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sm4vLMf8Hab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19ae49c-6c3b-4abe-e116-920654de704c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277814/277814 [1:01:17<00:00, 75.54it/s]\n"
          ]
        }
      ],
      "source": [
        "tokens = token_index(tokens, vocabulary)\n",
        "save_file(tokens_path, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create attention"
      ],
      "metadata": {
        "id": "4q5aZ36xepoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vec_len, seq_len, n_classes):\n",
        "        super(AttentionModel, self).__init__()\n",
        "        self.vec_len = vec_len\n",
        "        self.seq_len = seq_len\n",
        "        self.attn_weights = torch.cat([torch.tensor([[0.]]),\n",
        "                                       torch.randn(vec_len, 1) /\n",
        "                                       torch.sqrt(torch.tensor(vec_len))])\n",
        "        self.attn_weights.requires_grad = True\n",
        "        self.attn_weights = nn.Parameter(self.attn_weights)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.linear = nn.Linear(vec_len + 1, n_classes)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        hidden = torch.matmul(input_data, self.attn_weights)\n",
        "        hidden = self.activation(hidden)\n",
        "        attn = self.softmax(hidden)\n",
        "        attn = attn.repeat(1, 1, self.vec_len + 1).reshape(attn.shape[0],\n",
        "                                                           self.seq_len,\n",
        "                                                           self.vec_len + 1)\n",
        "        attn_output = input_data * attn\n",
        "        attn_output = torch.sum(attn_output, axis=1)\n",
        "        output = self.linear(attn_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "YqcuU6MFerxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create PyTorch dataset"
      ],
      "metadata": {
        "id": "PU_VBRw3ewMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokens, embeddings, labels):\n",
        "        \"\"\"\n",
        "        :param tokens: List of word tokens\n",
        "        :param embeddings: Word embeddings (from glove)\n",
        "        :param labels: List of labels\n",
        "        \"\"\"\n",
        "        self.tokens = tokens\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        emb = torch.tensor(self.embeddings[self.tokens[idx], :])\n",
        "        input_ = torch.cat((torch.ones(emb.shape[0],1), emb), dim=1)\n",
        "        return torch.tensor(self.labels[idx]), input_"
      ],
      "metadata": {
        "id": "7NSodmsHeyOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to train the model"
      ],
      "metadata": {
        "id": "xgL2bg2fe0BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "          device, num_epochs, model_path):\n",
        "    \"\"\"\n",
        "    Function to train the model\n",
        "    :param train_loader: Data loader for train dataset\n",
        "    :param valid_loader: Data loader for validation dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param optimizer: Optimizer\n",
        "    :param device: CUDA or CPU\n",
        "    :param num_epochs: Number of epochs\n",
        "    :param model_path: Path to save the model\n",
        "    \"\"\"\n",
        "    best_loss = 1e8\n",
        "    for i in range(num_epochs):\n",
        "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
        "        valid_loss, train_loss = [], []\n",
        "        model.train()\n",
        "        # Train loop\n",
        "        for batch_labels, batch_data in tqdm(train_loader):\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            batch_data = batch_data.to(device)\n",
        "            # Forward pass\n",
        "            batch_output = model(batch_data)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            train_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Gradient update step\n",
        "            optimizer.step()\n",
        "        model.eval()\n",
        "        # Validation loop\n",
        "        for batch_labels, batch_data in tqdm(valid_loader):\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            batch_data = batch_data.to(device)\n",
        "            # Forward pass\n",
        "            batch_output = model(batch_data)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            valid_loss.append(loss.item())\n",
        "        t_loss = np.mean(train_loss)\n",
        "        v_loss = np.mean(valid_loss)\n",
        "        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n",
        "        if v_loss < best_loss:\n",
        "            best_loss = v_loss\n",
        "            # Save model if validation loss improves\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "rAWwAXXje4Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to test the model"
      ],
      "metadata": {
        "id": "FVzHqE0oe-G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Function to test the model\n",
        "    :param test_loader: Data loader for test dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param device: CUDA or CPU\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    test_accu = []\n",
        "    for batch_labels, batch_data in tqdm(test_loader):\n",
        "        # Move data to device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        batch_data = batch_data.to(device)\n",
        "        # Forward pass\n",
        "        batch_output = model(batch_data)\n",
        "        batch_output = torch.squeeze(batch_output)\n",
        "        # Calculate loss\n",
        "        loss = criterion(batch_output, batch_labels)\n",
        "        test_loss.append(loss.item())\n",
        "        batch_preds = torch.argmax(batch_output, axis=1)\n",
        "        # Move predictions to CPU\n",
        "        if torch.cuda.is_available():\n",
        "            batch_labels = batch_labels.cpu()\n",
        "            batch_preds = batch_preds.cpu()\n",
        "        # Compute accuracy\n",
        "        test_accu.append(accuracy_score(batch_labels.detach().\n",
        "                                        numpy(),\n",
        "                                        batch_preds.detach().\n",
        "                                        numpy()))\n",
        "    test_loss = np.mean(test_loss)\n",
        "    test_accu = np.mean(test_accu)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accu}\")"
      ],
      "metadata": {
        "id": "MUqQQPofe_8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train attention model"
      ],
      "metadata": {
        "id": "Nsg3ISWBfEoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = load_file(tokens_path)\n",
        "labels = load_file(labels_path)\n",
        "embeddings = load_file(embeddings_path)\n",
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "vocabulary = load_file(vocabulary_path)"
      ],
      "metadata": {
        "id": "S3ZKLvZQfGl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train, validation and test sets"
      ],
      "metadata": {
        "id": "yBx-EJhQfIri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tokens, labels,\n",
        "                                                    test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
        "                                                      y_train,\n",
        "                                                      test_size=0.25)"
      ],
      "metadata": {
        "id": "ueE1MMWafNDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch datasets"
      ],
      "metadata": {
        "id": "K9iGIdA-fh3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(X_train, embeddings, y_train)\n",
        "valid_dataset = TextDataset(X_valid, embeddings, y_valid)\n",
        "test_dataset = TextDataset(X_test, embeddings, y_test)"
      ],
      "metadata": {
        "id": "LIomhLg6fjUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders"
      ],
      "metadata": {
        "id": "QdYGSv4dfmG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=16,\n",
        "                                           shuffle=True,\n",
        "                                           drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=16)"
      ],
      "metadata": {
        "id": "J3ogJn66fmpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model object"
      ],
      "metadata": {
        "id": "1JoDYyqVfo0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                      else \"cpu\")\n",
        "model = AttentionModel(vec_len, seq_len, num_classes)"
      ],
      "metadata": {
        "id": "YVgcSGwefqMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move the model to GPU if available\n"
      ],
      "metadata": {
        "id": "Ugqfbe4ZfzfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "metadata": {
        "id": "o2pFBtoJf5WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function and optimizer"
      ],
      "metadata": {
        "id": "tYHhAkJFf7da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "3MUue7WEf86a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "plbMM0Zhf-uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "      device, num_epochs, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQDmHWXEgAbI",
        "outputId": "81ae2215-867c-418f-999a-6edecb7aed2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:25<00:00, 414.90it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 841.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.255738702538856, Validation Loss: 1.1066387673379225\n",
            "Best Validation Loss: 1.1066387673379225\n",
            "Epoch 2 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 421.23it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 824.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0755504596385554, Validation Loss: 1.056090299244954\n",
            "Best Validation Loss: 1.056090299244954\n",
            "Epoch 3 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.14it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 834.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0446229579267219, Validation Loss: 1.038802680006777\n",
            "Best Validation Loss: 1.038802680006777\n",
            "Epoch 4 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.43it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 824.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0319891958444722, Validation Loss: 1.0285619565776745\n",
            "Best Validation Loss: 1.0285619565776745\n",
            "Epoch 5 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.73it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 716.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0246236741680128, Validation Loss: 1.023001933102988\n",
            "Best Validation Loss: 1.023001933102988\n",
            "Epoch 6 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 432.65it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 673.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0191699383520398, Validation Loss: 1.0177368418716608\n",
            "Best Validation Loss: 1.0177368418716608\n",
            "Epoch 7 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 434.77it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 712.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0095845026400867, Validation Loss: 1.005208261257422\n",
            "Best Validation Loss: 1.005208261257422\n",
            "Epoch 8 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.90it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 818.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0016892412489677, Validation Loss: 0.9995055579071666\n",
            "Best Validation Loss: 0.9995055579071666\n",
            "Epoch 9 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.66it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 830.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.997764420590295, Validation Loss: 0.9967921788037561\n",
            "Best Validation Loss: 0.9967921788037561\n",
            "Epoch 10 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 418.29it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 832.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9950216556378073, Validation Loss: 0.9944318123279784\n",
            "Best Validation Loss: 0.9944318123279784\n",
            "Epoch 11 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.87it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 818.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.993113871272614, Validation Loss: 0.9930578821284813\n",
            "Best Validation Loss: 0.9930578821284813\n",
            "Epoch 12 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.06it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 784.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.991750494912939, Validation Loss: 0.9922084847822884\n",
            "Best Validation Loss: 0.9922084847822884\n",
            "Epoch 13 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 428.24it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 692.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9905621492318032, Validation Loss: 0.9909086845602798\n",
            "Best Validation Loss: 0.9909086845602798\n",
            "Epoch 14 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 438.33it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 670.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9896454509683089, Validation Loss: 0.9898014178098947\n",
            "Best Validation Loss: 0.9898014178098947\n",
            "Epoch 15 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 432.93it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 729.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9888484041575027, Validation Loss: 0.9888712290192831\n",
            "Best Validation Loss: 0.9888712290192831\n",
            "Epoch 16 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.59it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 830.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.988181118942607, Validation Loss: 0.988128582544983\n",
            "Best Validation Loss: 0.988128582544983\n",
            "Epoch 17 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.44it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 838.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9875622561223266, Validation Loss: 0.9882779765928852\n",
            "Best Validation Loss: 0.988128582544983\n",
            "Epoch 18 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.16it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 830.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9869490313504794, Validation Loss: 0.987378104143928\n",
            "Best Validation Loss: 0.987378104143928\n",
            "Epoch 19 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.87it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 828.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.986495810304317, Validation Loss: 0.9879264991838789\n",
            "Best Validation Loss: 0.987378104143928\n",
            "Epoch 20 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.65it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 781.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9860955294093625, Validation Loss: 0.9874426510915474\n",
            "Best Validation Loss: 0.987378104143928\n",
            "Epoch 21 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 429.84it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 687.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.985507749786279, Validation Loss: 0.9865405066896165\n",
            "Best Validation Loss: 0.9865405066896165\n",
            "Epoch 22 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 441.15it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 661.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9853471016976535, Validation Loss: 0.9854999168904961\n",
            "Best Validation Loss: 0.9854999168904961\n",
            "Epoch 23 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 430.94it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 762.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9848421458794989, Validation Loss: 0.986618501821513\n",
            "Best Validation Loss: 0.9854999168904961\n",
            "Epoch 24 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.38it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 834.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9845013365773121, Validation Loss: 0.9849014830819269\n",
            "Best Validation Loss: 0.9849014830819269\n",
            "Epoch 25 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.81it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 829.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9843180471525972, Validation Loss: 0.9852400704845036\n",
            "Best Validation Loss: 0.9849014830819269\n",
            "Epoch 26 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.25it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 837.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9838481827494759, Validation Loss: 0.9859165395504111\n",
            "Best Validation Loss: 0.9849014830819269\n",
            "Epoch 27 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.41it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 822.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9835943459197206, Validation Loss: 0.9849867461447135\n",
            "Best Validation Loss: 0.9849014830819269\n",
            "Epoch 28 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.47it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 776.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9834078643839312, Validation Loss: 0.9845672782511592\n",
            "Best Validation Loss: 0.9845672782511592\n",
            "Epoch 29 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.98it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 677.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9831553827331148, Validation Loss: 0.9836875126302123\n",
            "Best Validation Loss: 0.9836875126302123\n",
            "Epoch 30 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 435.57it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 685.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9828436924525451, Validation Loss: 0.9833767315495643\n",
            "Best Validation Loss: 0.9833767315495643\n",
            "Epoch 31 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.36it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 804.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9827612449984835, Validation Loss: 0.9834930572134255\n",
            "Best Validation Loss: 0.9833767315495643\n",
            "Epoch 32 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.78it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 825.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.98231158369069, Validation Loss: 0.9837551246175428\n",
            "Best Validation Loss: 0.9833767315495643\n",
            "Epoch 33 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.95it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 814.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9819662333046848, Validation Loss: 0.9834405564022723\n",
            "Best Validation Loss: 0.9833767315495643\n",
            "Epoch 34 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.76it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 832.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9818470791657361, Validation Loss: 0.9828674562925431\n",
            "Best Validation Loss: 0.9828674562925431\n",
            "Epoch 35 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 427.02it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 772.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9816579968938673, Validation Loss: 0.9825234037721806\n",
            "Best Validation Loss: 0.9825234037721806\n",
            "Epoch 36 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.48it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 688.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9814110343897152, Validation Loss: 0.9827808590244911\n",
            "Best Validation Loss: 0.9825234037721806\n",
            "Epoch 37 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 435.12it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 665.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.981285727414272, Validation Loss: 0.981840180228294\n",
            "Best Validation Loss: 0.981840180228294\n",
            "Epoch 38 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.31it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 770.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9809175679432635, Validation Loss: 0.9828504261145357\n",
            "Best Validation Loss: 0.981840180228294\n",
            "Epoch 39 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 425.41it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 827.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9808033344586697, Validation Loss: 0.9825022798809235\n",
            "Best Validation Loss: 0.981840180228294\n",
            "Epoch 40 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.45it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 827.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9806233123173684, Validation Loss: 0.9822048930348837\n",
            "Best Validation Loss: 0.981840180228294\n",
            "Epoch 41 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.41it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 828.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9804581178941889, Validation Loss: 0.9818949001325574\n",
            "Best Validation Loss: 0.981840180228294\n",
            "Epoch 42 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 422.20it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 769.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9802612818221164, Validation Loss: 0.9808651503210895\n",
            "Best Validation Loss: 0.9808651503210895\n",
            "Epoch 43 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 429.51it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 678.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9799339538928208, Validation Loss: 0.9816217129267569\n",
            "Best Validation Loss: 0.9808651503210895\n",
            "Epoch 44 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:23<00:00, 440.67it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 688.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9798378546350884, Validation Loss: 0.9818789867736268\n",
            "Best Validation Loss: 0.9808651503210895\n",
            "Epoch 45 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 429.41it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 796.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9796481972853861, Validation Loss: 0.9818676761186607\n",
            "Best Validation Loss: 0.9808651503210895\n",
            "Epoch 46 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.46it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 833.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9794733141452205, Validation Loss: 0.9801921488450647\n",
            "Best Validation Loss: 0.9801921488450647\n",
            "Epoch 47 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 426.98it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 826.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9792804466208265, Validation Loss: 0.9801879286628754\n",
            "Best Validation Loss: 0.9801879286628754\n",
            "Epoch 48 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 423.46it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 817.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9791590079999881, Validation Loss: 0.9805058835306533\n",
            "Best Validation Loss: 0.9801879286628754\n",
            "Epoch 49 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 424.94it/s]\n",
            "100%|██████████| 3473/3473 [00:04<00:00, 790.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9791298950429634, Validation Loss: 0.9797732829790173\n",
            "Best Validation Loss: 0.9797732829790173\n",
            "Epoch 50 of 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10418/10418 [00:24<00:00, 432.44it/s]\n",
            "100%|██████████| 3473/3473 [00:05<00:00, 686.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9788074452300565, Validation Loss: 0.9800696365262808\n",
            "Best Validation Loss: 0.9797732829790173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model"
      ],
      "metadata": {
        "id": "utvsL_F7gDzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_loader, model, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khNr77ZygFVI",
        "outputId": "49fd01a8-8a4d-4c68-cfb8-b6559a3426f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3473/3473 [00:07<00:00, 495.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9828092007928096, Test Accuracy: 0.6694434992016335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict on new text"
      ],
      "metadata": {
        "id": "BD-2KEvZgHvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = '''I am a victim of Identity Theft & currently have an Experian account that\n",
        "I can view my Experian Credit Report and getting notified when there is activity on\n",
        "my Experian Credit Report. For the past 3 days I've spent a total of approximately 9\n",
        "hours on the phone with Experian. Every time I call I get transferred repeatedly and\n",
        "then my last transfer and automated message states to press 1 and leave a message and\n",
        "someone would call me. Every time I press 1 I get an automatic message stating than you\n",
        "before I even leave a message and get disconnected. I call Experian again, explain what\n",
        "is happening and the process begins again with the same end result. I was trying to have\n",
        "this issue attended and resolved informally but I give up after 9 hours. There are hard\n",
        "hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall\n",
        "and I respectfully request that Experian remove the hard hit inquiries immediately just\n",
        "like they've done in the past when I was able to speak to a live Experian representative\n",
        "in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX\n",
        "XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX\n",
        "XX/XX/XXXX'''"
      ],
      "metadata": {
        "id": "w-mQIBpLgIdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process input text"
      ],
      "metadata": {
        "id": "Vs0qechPgK9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input_text.lower()\n",
        "input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n",
        "input_text = re.sub(\"\\d+\", \"\", input_text)\n",
        "input_text = re.sub(r'[x]{2,}', \"\", input_text)\n",
        "input_text = re.sub(' +', ' ', input_text)\n",
        "tokens = word_tokenize(input_text)"
      ],
      "metadata": {
        "id": "Gtb9JaN-gMeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = ['<pad>']*(20-len(tokens))+tokens"
      ],
      "metadata": {
        "id": "woepmNhBgORQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_token = []\n",
        "for token in tokens:\n",
        "    if token in vocabulary:\n",
        "        idx_token.append(vocabulary.index(token))\n",
        "    else:\n",
        "        idx_token.append(vocabulary.index('<unk>'))"
      ],
      "metadata": {
        "id": "j6QhkvQ8gPGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_emb = embeddings[idx_token,:]\n",
        "token_emb = token_emb[:seq_len, :]\n",
        "inp = torch.from_numpy(token_emb)"
      ],
      "metadata": {
        "id": "cZjZGz5vgQiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.cat((torch.ones(inp.shape[0],1), inp), dim=1)"
      ],
      "metadata": {
        "id": "bDjIxY2agSKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                      else \"cpu\")"
      ],
      "metadata": {
        "id": "YTg3_m9ngUi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = inp.to(device)\n",
        "inp = torch.unsqueeze(inp, 0)"
      ],
      "metadata": {
        "id": "fmK8gR-JgW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "hwBRh9MsgY2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model object\n",
        "model = AttentionModel(vec_len, seq_len, num_classes)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Forward pass\n",
        "out = torch.squeeze(model(inp))\n",
        "\n",
        "# Find predicted class\n",
        "prediction = label_encoder.classes_[torch.argmax(out)]\n",
        "print(f\"Predicted  Class: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG5fASM5gahA",
        "outputId": "3324283c-80ed-4cfd-99cb-cbbb6d091365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted  Class: credit_report\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-c6eeda1fc3f5>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save model"
      ],
      "metadata": {
        "id": "5LYNaQxTqrnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def save_model(model, model_path):\n",
        "    \"\"\"\n",
        "    Saves the model to a file.\n",
        "\n",
        "    Args:\n",
        "        model: The model to save.\n",
        "        model_path: The path to save the model to.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "def load_model(model, model_path):\n",
        "    \"\"\"\n",
        "    Loads the model from a file.\n",
        "\n",
        "    Args:\n",
        "        model: The model to load.\n",
        "        model_path: The path to load the model from.\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Model loaded from {model_path}\")"
      ],
      "metadata": {
        "id": "oZWtmYTgqrZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/TextClassificationAttention/models\"\n",
        "# Create an instance of the model\n",
        "model_instance = AttentionModel(vec_len, seq_len, num_classes)\n",
        "save_model(model_instance, model_path)  # Save the model instance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNflyh9RqwbI",
        "outputId": "06a2ac25-810b-45b6-8e3c-c5576e261a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/TextClassificationAttention/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionModel(vec_len, seq_len, num_classes)  # Create the model instance\n",
        "load_model(model, model_path)  # Load the saved weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJgYwJdqxzH",
        "outputId": "a8598e23-98af-444a-e5ce-96aba0faf45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/TextClassificationAttention/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-ff72fdc4ab4f>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2uwoUzvvnvn6",
        "DleSFNb57lKD",
        "QTz5jq6Q5lfE",
        "emAUjvb36Mh8",
        "bPuy4-Ax6vs6",
        "rosMMWOA63eE",
        "poZ7ioFL7Hu3",
        "QU4nPMHf7rl6",
        "BP16KhR27wcr",
        "8MIRMH4k70s7",
        "PTiWqkj277Si",
        "mAU4GSVI8Byz",
        "1PovRhgP_mXc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}